Lab1:
I used the capabilites (e.g. HoughCircles, Contours) within OpenCV to locate an object using real-time images streaming on the computer.

Lab2:
In this lab, I created a ROS2 package containing two communicating nodes that enable the robot to turn in place to follow an object.

Lab3:
The objective of this lab was to explore PID control and combine multiple sensor inputs. The goal was to have the robot chase a desired object observed in its local coordinate frame. Specifically, I made the robot always face the object and maintain a desired distance from the object. 

Lab 4:
![Screenshot 2023-12-17 000354](https://github.com/AbirathR/Turtlebot3-Burger/assets/67257433/e9189e9a-62f4-4226-a9b5-9129d7021d4c)

Lab5:
The objective of this lab was to get familiar with the mapping, localization and path-planning capabilities of the ROS2 navigation stack. The final goal was to write a script to make a Gazebo-simulated and real robot autonomously navigate to a series of global waypoints.

Lab 6:
The objective of the final project was to integrate multi-modal sensing and navigation into a single reasoning system. The goal was for the robot to complete a maze by following signs. 
